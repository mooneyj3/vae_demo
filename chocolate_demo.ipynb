{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Demo with Chocolate\n",
    "\n",
    "This was adapted from the VAE demo available from PyTorch: https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "\n",
    "**Chocolate**: https://github.com/AIworx-Labs/chocolate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import chocolate as choco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a connection to MongoDB\n",
    "\n",
    "Establish the connection to a running mongodb instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database connection info\n",
    "DATABASE_URL = '140.160.139.44:27017'\n",
    "DATABASE_NAME = 'choco_demo'\n",
    "\n",
    "# chocolate setup\n",
    "choco_conn = choco.MongoDBConnection(url=DATABASE_URL, database=DATABASE_NAME)\n",
    "choco_conn.clear()  # clear the database for new experiment runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbd37db5b90>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_gpu = False\n",
    "\n",
    "device = torch.device(\"cuda\" if on_gpu else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if on_gpu else {}\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Setup\n",
    "\n",
    "This is setting up the model and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function:\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.ToTensor()),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop and Test Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "#         if batch_idx % log_interval == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader),\n",
    "#                 loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams the worst way\n",
    "\n",
    "Normally, we might use argparse or some other mechanism to drive our experiment.\n",
    "\n",
    "Here, I'm just going to hardcode the hyperparams, which is probably the worst way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "log_interval = 200\n",
    "learning_rate =1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_loader, test_loader = load_datasets(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 164.1720\n",
      "====> Test set loss: 127.7911\n",
      "====> Epoch: 2 Average loss: 121.6052\n",
      "====> Test set loss: 115.7066\n",
      "====> Epoch: 3 Average loss: 114.4285\n",
      "====> Test set loss: 111.5441\n",
      "====> Epoch: 4 Average loss: 111.4300\n",
      "====> Test set loss: 109.3296\n",
      "====> Epoch: 5 Average loss: 109.6984\n",
      "====> Test set loss: 108.2240\n",
      "====> Epoch: 6 Average loss: 108.5445\n",
      "====> Test set loss: 107.5437\n",
      "====> Epoch: 7 Average loss: 107.6886\n",
      "====> Test set loss: 106.7517\n",
      "====> Epoch: 8 Average loss: 107.0719\n",
      "====> Test set loss: 106.1587\n",
      "====> Epoch: 9 Average loss: 106.5827\n",
      "====> Test set loss: 105.8837\n",
      "====> Epoch: 10 Average loss: 106.1514\n",
      "====> Test set loss: 105.7077\n"
     ]
    }
   ],
   "source": [
    "# Do stuff\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's have some Chocolate\n",
    "\n",
    "We'll start by creating our hyperparameter space.\n",
    "\n",
    "Note:  we can create different range types for the hyperparameter space.  This includes:\n",
    "* `uniform`: Uniform continuous distribution.\n",
    "* `quantized_uniform`: Uniform discrete distribution.\n",
    "* `log`: Logarithmic uniform continuous distribution.\n",
    "* `quantized_log`: Logarithmic uniform discrete distribution.\n",
    "* `choice`: Uniform choice distribution between non-numeric samples.\n",
    "\n",
    "For more details on how these spaces are computed: https://chocolate.readthedocs.io/api/space.html#module-chocolate.space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_space():\n",
    "    space = {\n",
    "        \"learning_rate\": choco.log(low=-4, high=-1, base=10),\n",
    "        \"batch_size\": choco.choice([16, 32, 64, 128, 256, 512]),\n",
    "        \"epochs\": choco.quantized_uniform(low=10, high=100, step=5)\n",
    "    }\n",
    "    return space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, setup the space and define the tuning algorithm\n",
    "\n",
    "Chocolate offers the following sampling algorithms:\n",
    "* `Grid`: Regular cartesian grid sampler.\n",
    "* `Random`: Random sampler.\n",
    "* `QuasiRandom`: Quasi-Random sampler. Samples the search space using the generalized Halton low-discrepancy sequence. \n",
    "\n",
    "Chocolate offers the following search algorithms:\n",
    "* `Bayes`: Bayesian minimization method with gaussian process regressor.\n",
    "* `CMAES`: Covariance Matrix Adaptation Evolution Strategy minimization method.\n",
    "* `MOCMAES`: Multi-Objective Covariance Matrix Adaptation Evolution Strategy.\n",
    "\n",
    "For this demonstration, we'll use `Bayes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "choco_space = create_space()\n",
    "sampler = choco.Bayes(choco_conn, choco_space, clear_db=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets do a simple training loop with Chocolate requesting a configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_chocolate_id': 1}\n",
      "{'learning_rate': 0.00023468606397270453, 'epochs': 45, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "# Have the choco sampler fetch parameters\n",
    "tokens, params = sampler.next()\n",
    "\n",
    "# Display the chocolate space\n",
    "print(tokens)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 125.9546\n",
      "====> Test set loss: 112.1922\n",
      "====> Epoch: 2 Average loss: 111.3176\n",
      "====> Test set loss: 109.3802\n",
      "====> Epoch: 3 Average loss: 109.4607\n",
      "====> Test set loss: 108.1772\n",
      "====> Epoch: 4 Average loss: 108.4805\n",
      "====> Test set loss: 107.4520\n",
      "====> Epoch: 5 Average loss: 107.7983\n",
      "====> Test set loss: 106.9426\n",
      "====> Epoch: 6 Average loss: 107.2659\n",
      "====> Test set loss: 106.4481\n",
      "====> Epoch: 7 Average loss: 106.8209\n",
      "====> Test set loss: 106.2418\n",
      "====> Epoch: 8 Average loss: 106.5144\n",
      "====> Test set loss: 105.8120\n",
      "====> Epoch: 9 Average loss: 106.2268\n",
      "====> Test set loss: 105.6413\n",
      "====> Epoch: 10 Average loss: 106.0182\n",
      "====> Test set loss: 105.4409\n",
      "====> Epoch: 11 Average loss: 105.7907\n",
      "====> Test set loss: 105.2756\n",
      "====> Epoch: 12 Average loss: 105.6666\n",
      "====> Test set loss: 105.1655\n",
      "====> Epoch: 13 Average loss: 105.5220\n",
      "====> Test set loss: 105.0749\n",
      "====> Epoch: 14 Average loss: 105.3564\n",
      "====> Test set loss: 104.8799\n",
      "====> Epoch: 15 Average loss: 105.2073\n",
      "====> Test set loss: 104.7609\n",
      "====> Epoch: 16 Average loss: 105.0822\n",
      "====> Test set loss: 104.6464\n",
      "====> Epoch: 17 Average loss: 104.9113\n",
      "====> Test set loss: 104.5904\n",
      "====> Epoch: 18 Average loss: 104.8326\n",
      "====> Test set loss: 104.5273\n",
      "====> Epoch: 19 Average loss: 104.7454\n",
      "====> Test set loss: 104.3193\n",
      "====> Epoch: 20 Average loss: 104.6405\n",
      "====> Test set loss: 104.3368\n",
      "====> Epoch: 21 Average loss: 104.5349\n",
      "====> Test set loss: 104.2327\n",
      "====> Epoch: 22 Average loss: 104.5000\n",
      "====> Test set loss: 104.1788\n",
      "====> Epoch: 23 Average loss: 104.3797\n",
      "====> Test set loss: 104.1080\n",
      "====> Epoch: 24 Average loss: 104.2843\n",
      "====> Test set loss: 104.0939\n",
      "====> Epoch: 25 Average loss: 104.2192\n",
      "====> Test set loss: 104.0088\n",
      "====> Epoch: 26 Average loss: 104.1749\n",
      "====> Test set loss: 103.8889\n",
      "====> Epoch: 27 Average loss: 104.1071\n",
      "====> Test set loss: 103.7721\n",
      "====> Epoch: 28 Average loss: 104.0201\n",
      "====> Test set loss: 103.7536\n",
      "====> Epoch: 29 Average loss: 104.0022\n",
      "====> Test set loss: 103.6950\n",
      "====> Epoch: 30 Average loss: 103.9053\n",
      "====> Test set loss: 103.6366\n",
      "====> Epoch: 31 Average loss: 103.8220\n",
      "====> Test set loss: 103.6162\n",
      "====> Epoch: 32 Average loss: 103.7694\n",
      "====> Test set loss: 103.5516\n",
      "====> Epoch: 33 Average loss: 103.7361\n",
      "====> Test set loss: 103.6125\n",
      "====> Epoch: 34 Average loss: 103.6957\n",
      "====> Test set loss: 103.7055\n",
      "====> Epoch: 35 Average loss: 103.5991\n",
      "====> Test set loss: 103.4799\n",
      "====> Epoch: 36 Average loss: 103.6109\n",
      "====> Test set loss: 103.5131\n",
      "====> Epoch: 37 Average loss: 103.5186\n",
      "====> Test set loss: 103.3344\n",
      "====> Epoch: 38 Average loss: 103.5099\n",
      "====> Test set loss: 103.2374\n",
      "====> Epoch: 39 Average loss: 103.4330\n",
      "====> Test set loss: 103.2794\n",
      "====> Epoch: 40 Average loss: 103.4029\n",
      "====> Test set loss: 103.3141\n",
      "====> Epoch: 41 Average loss: 103.3551\n",
      "====> Test set loss: 103.2483\n",
      "====> Epoch: 42 Average loss: 103.2901\n",
      "====> Test set loss: 103.2283\n",
      "====> Epoch: 43 Average loss: 103.2517\n",
      "====> Test set loss: 103.1320\n",
      "====> Epoch: 44 Average loss: 103.2447\n",
      "====> Test set loss: 103.1998\n"
     ]
    }
   ],
   "source": [
    "# Use our new hyperparameters from chocolate\n",
    "\n",
    "learning_rate = params['learning_rate']\n",
    "epochs = params['epochs']\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "train_loader, test_loader = load_datasets(batch_size)\n",
    "\n",
    "# set the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# run the training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        test_loss = test(epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results/sample_' + str(epoch) + '.png')\n",
    "\n",
    "\n",
    "# IMPORTANT!!!!            \n",
    "# Record the loss\n",
    "sampler.update(token, test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about a batch of experiments?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I do not reccommend running this code in Jupyter with on_gpu=False\n",
    "num_experiments = 10 if on_gpu else -1\n",
    "\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    \n",
    "    # get next configuration from chocolate\n",
    "    tokens, params = sampler.next()\n",
    "    \n",
    "    # Use our new hyperparameters from chocolate\n",
    "    learning_rate = params['learning_rate']\n",
    "    epochs = params['epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    \n",
    "    # Load the datasets\n",
    "    train_loader, test_loader = load_datasets(batch_size)\n",
    "\n",
    "    # set the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # run the training loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test_loss = test(epoch)\n",
    "            with torch.no_grad():\n",
    "                sample = torch.randn(64, 20).to(device)\n",
    "                sample = model.decode(sample).cpu()\n",
    "                save_image(sample.view(64, 1, 28, 28),\n",
    "                           'results/sample_' + str(epoch) + '.png')\n",
    "    \n",
    "    # Record the loss\n",
    "    sampler.update(token, test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra info\n",
    "\n",
    "### Chocolate and Sacred\n",
    "Chocolate works nice with Sacred since the params are returned in a dictionary form, you can load these directly into\n",
    "```python\n",
    "token, params = sampler.next()\n",
    "loss = ex.run(config_updates=params).result\n",
    "sampler.update(token, loss)\n",
    "\n",
    "```\n",
    "\n",
    "### Database Caveats\n",
    "If you change the `create_space` method, this requires the `clear_db` flag to be set to true, **OR** referencing a new database_name\n",
    "\n",
    "```python\n",
    "sampler = choco.Bayes(choco_conn, choco_space, clear_db=True)\n",
    "```\n",
    "\n",
    "```python\n",
    "# database connection info\n",
    "DATABASE_URL = '140.160.139.44:27017'\n",
    "DATABASE_NAME = 'choco_experiment2'  # note the new database name here\n",
    "\n",
    "# chocolate setup\n",
    "choco_conn = choco.MongoDBConnection(url=DATABASE_URL, database=DATABASE_NAME)\n",
    "choco_conn.clear()  # clear the database for new experiment runs\n",
    "```\n",
    "\n",
    "One of the downsides of Chocolate, is that each space is unique and requires it's own database.  While the footprint of a database is relatively small, this can add up if you need to keep a long history of different experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
